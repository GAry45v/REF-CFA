# æ–‡ä»¶: ddad_transformer.py
import torch
from sklearn.metrics import roc_auc_score, average_precision_score, classification_report
from typing import Any
import torch
from torchvision import transforms
import os
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
from matplotlib.font_manager import FontProperties

from unet import *
from visualize_steps import *
from reconstruction import Reconstruction
from metrics import *
from transformer_classifier import ClassificationTransformer

from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget

import cv2
from scipy.ndimage import gaussian_filter
from torch.optim import Adam

from pathlib import Path

import importlib  # <--- (æ–°) æ·»åŠ è¿™ä¸€è¡Œ

# ... (æ‰€æœ‰ import ä¹‹å)

def _tensor_to_cv2_image(tensor_img: torch.Tensor) -> np.ndarray:
    """
    å°† [C, H, W]ã€èŒƒå›´ [-1, 1] çš„ PyTorch Tensor è½¬æ¢ä¸º [H, W, 3]ã€
    èŒƒå›´ [0, 255] çš„ BGR (OpenCV) å›¾åƒã€‚
    """
    img_np = tensor_img.cpu().numpy().transpose(1, 2, 0) # H, W, C
    # åå½’ä¸€åŒ– (å‡è®¾ä½ çš„ transform æ˜¯ Normalize(0.5, 0.5))
    img_np = (img_np * 0.5 + 0.5) * 255 
    img_np = img_np.astype(np.uint8)
    
    if img_np.shape[2] == 1:
        # ä»ç°åº¦å›¾è½¬ä¸º BGR
        img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR) 
    elif img_np.shape[2] == 3:
        # ä» RGB (PyTorch/plt) è½¬ä¸º BGR (OpenCV)
        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR) 
        
    return img_np.copy()

def _cv2_image_to_plt(cv2_img_bgr: np.ndarray) -> np.ndarray:
    """å°† [H, W, 3] çš„ BGR (OpenCV) å›¾åƒè½¬æ¢ä¸º RGB (Matplotlib) å›¾åƒã€‚"""
    return cv2.cvtColor(cv2_img_bgr, cv2.COLOR_BGR2RGB)

# --- å¯ä»¥æ·»åŠ åœ¨ ddad_transformer.py çš„é¡¶éƒ¨ ---
class LocalizationSegmenter(nn.Module):
    """
    ä¸€ä¸ªç®€å•çš„å…¨å·ç§¯ç½‘ç»œ (FCN)ï¼Œç”¨äºä»èšåˆçš„ diff_map é¢„æµ‹å¼‚å¸¸æ©ç ã€‚
    å®ƒå‡è®¾è¾“å…¥æ˜¯ [B, 1, H, W] çš„èšåˆç‰¹å¾å›¾ã€‚
    """
    def __init__(self, in_channels=1, out_channels=1):
        super(LocalizationSegmenter, self).__init__()
        # ç¼–ç å™¨
        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        
        # ç“¶é¢ˆ
        self.bottleneck = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # è§£ç å™¨
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)
        self.upconv2 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)
        
        # è¾“å‡ºå±‚
        self.conv_out = nn.Conv2d(16, out_channels, kernel_size=1)

    def forward(self, x):
        # ç¼–ç 
        x1 = F.relu(self.conv1(x))
        x2 = F.relu(self.conv2(self.pool(x1)))
        
        # ç“¶é¢ˆ
        x_bottle = F.relu(self.bottleneck(self.pool(x2)))
        
        # è§£ç 
        x_up1 = self.upconv1(x_bottle)
        x_up1 = F.relu(self.conv3(x_up1))
        
        x_up2 = self.upconv2(x_up1)
        
        # è¾“å‡º
        # æˆ‘ä»¬ä½¿ç”¨ logits (åŸå§‹å€¼) ä½œä¸ºè¾“å‡º, æŸå¤±å‡½æ•°å°†å¤„ç† sigmoid
        logits = self.conv_out(x_up2) 
        return logits

# --- æ”¾åœ¨ DDAD_Transformer_Analysis åŒçº§åˆ«çš„ä½ç½® ---

class DiceBCELoss(nn.Module):
    """ ç»“åˆ Dice æŸå¤±å’Œ BCE æŸå¤±ï¼Œæ›´ç¨³å®š """
    def __init__(self, weight=None, size_average=True):
        super(DiceBCELoss, self).__init__()

    def forward(self, inputs_logits, targets, smooth=1e-6):
        # inputs_logits æ˜¯æ¨¡å‹çš„åŸå§‹è¾“å‡º (logits)
        inputs = torch.sigmoid(inputs_logits)       
        
        # --- BCE Loss ---
        bce_loss = F.binary_cross_entropy_with_logits(inputs_logits, targets, reduction='mean')
        
        # --- Dice Loss ---
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()                            
        dice_loss = 1 - (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)
        
        # ç»“åˆä¸¤ç§æŸå¤±
        return bce_loss + dice_loss

class SupervisedLocalizationModule:
    def __init__(self, reconstruction_module: Reconstruction, config, device):
        self.reconstruction = reconstruction_module
        self.config = config
        self.device = device
        
        # å‡è®¾æˆ‘ä»¬èšåˆ diff_maps ä¸º 1 ä¸ªé€šé“
        self.segmenter = LocalizationSegmenter(in_channels=1, out_channels=1).to(device)
        
        self.optimizer = Adam(self.segmenter.parameters(), lr=1e-4)
        self.loss_fn = DiceBCELoss() # ä½¿ç”¨ Dice+BCE æŸå¤±

        # æ£€æŸ¥ç‚¹è·¯å¾„
        clean_data_name = self.config.data.category.split("_")[0]
        self.checkpoint_dir = os.path.join(os.getcwd(), self.config.model.checkpoint_dir, clean_data_name)
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.segmenter_checkpoint_path = "/data/xjy/DDAD/checkpoints_official/bottle/supervised_segmenter.pth"
    def _get_aggregated_map(self, input_data, w_localization=3, use_final_residual=False):
        """
        (å¤ç”¨é€»è¾‘) ä»é‡å»ºæ¨¡å—è·å–ç”¨äºåˆ†å‰²çš„ç‰¹å¾å›¾ã€‚
        """
        use_final_residual = True
        w_localization = 3
        self.reconstruction.unet.eval()
        with torch.no_grad():
            final_recon, _, _, diff_maps, _ = self.reconstruction(
                input_data, 
                input_data, 
                w_localization
            )
        
        if use_final_residual:
            # ç­–ç•¥1: ä½¿ç”¨æœ€ç»ˆçš„æ®‹å·®
            residual_map = torch.abs(final_recon - input_data)
            aggregated_map = torch.mean(residual_map, dim=1, keepdim=True) # [B, 1, H, W]
        else:
            # ç­–ç•¥2: ä½¿ç”¨ Diff maps çš„å‡å€¼
            abs_diff_maps = [torch.abs(d) for d in diff_maps]
            diff_stack = torch.stack(abs_diff_maps, dim=0) 
            aggregated_map_time = torch.mean(diff_stack, dim=0) # [B, C, H, W]
            aggregated_map = torch.mean(aggregated_map_time, dim=1, keepdim=True) # [B, 1, H, W]
            
        return aggregated_map

    def train(self, source_dataloader, num_epochs=20):
        """
        åœ¨æºåŸŸä¸Šè®­ç»ƒåˆ†å‰²å™¨ã€‚
        source_dataloader å¿…é¡»è¿”å› (images, masks, ...)
        """
        print(f"--- ğŸš€ å¼€å§‹æœ‰ç›‘ç£å®šä½æ¨¡å‹è®­ç»ƒ (æºåŸŸ) ---")
        
        # ç¡®ä¿é‡å»ºæ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼ï¼Œæˆ‘ä»¬åªè®­ç»ƒ segmenter
        self.reconstruction.unet.eval() 
        self.segmenter.train()
        
        for epoch in range(num_epochs):
            total_loss = 0
            for batch in source_dataloader:
                # å‡è®¾ dataloader è¿”å› (image, mask, label)
                # æ‚¨éœ€è¦ä¿®æ”¹æ‚¨çš„ Dataset_maker æ¥åŠ è½½ mask
                images, masks, _ = batch 
                images = images.to(self.device)
                masks = masks.to(self.device) # [B, 1, H, W]

                # 1. è·å–ç‰¹å¾
                # æˆ‘ä»¬åœ¨è®­ç»ƒæ—¶ä¹Ÿä½¿ç”¨ no_gradï¼Œå› ä¸ºæˆ‘ä»¬ä¸è®­ç»ƒ UNet
                feature_map = self._get_aggregated_map(images)
                
                # 2. é¢„æµ‹æ©ç 
                self.optimizer.zero_grad()
                pred_logits = self.segmenter(feature_map)
                
                # 3. è®¡ç®—æŸå¤±
                # ç¡®ä¿ mask å’Œ pred_logits å°ºå¯¸åŒ¹é…
                # (å¦‚æœéœ€è¦ï¼Œè°ƒæ•´ mask å¤§å°)
                if pred_logits.shape[2:] != masks.shape[2:]:
                    masks = F.interpolate(masks, size=pred_logits.shape[2:], mode='nearest')
                
                loss = self.loss_fn(pred_logits, masks)
                
                # 4. åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
                
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Loss: {total_loss / len(source_dataloader):.4f}")
        
        # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        torch.save(self.segmenter.state_dict(), self.segmenter_checkpoint_path)
        print(f"--- âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {self.segmenter_checkpoint_path} ---")

    def localize_on_target_domain(self, target_dataloader, num_samples=10, threshold=0.5):
        """
        åœ¨ç›®æ ‡åŸŸä¸Šè¿è¡Œæ¨ç†å’Œå¯è§†åŒ–ã€‚
        target_dataloader åªéœ€è¦è¿”å› (images, ...)
        """
        print(f"--- ğŸš€ å¼€å§‹åœ¨ç›®æ ‡åŸŸä¸Šè¿›è¡Œå¼‚å¸¸å®šä½ (æ¨ç†) ---")
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        if not os.path.exists(self.segmenter_checkpoint_path):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ {self.segmenter_checkpoint_path}")
            print("è¯·å…ˆè°ƒç”¨ .train() æ–¹æ³•åœ¨æºåŸŸä¸Šè¿›è¡Œè®­ç»ƒã€‚")
            return
            
        self.segmenter.load_state_dict(torch.load(self.segmenter_checkpoint_path, map_location=self.device))
        self.segmenter.eval()
        self.reconstruction.unet.eval()
        
        output_dir = 'supervised_localization_results'
        os.makedirs(output_dir, exist_ok=True)
        
        processed_samples = 0
        
        # (å¤ç”¨ DDAD_Transformer_Analysis ä¸­çš„è¾…åŠ©å‡½æ•°)
        ddad_helper = self.reconstruction.unet # å€Ÿç”¨ä¸€ä¸ªå®ä¾‹æ¥è®¿é—®æ–¹æ³•
        
        with torch.no_grad():
            for i, (input_data, _, labels) in enumerate(target_dataloader):
                if processed_samples >= num_samples:
                    break
                
                original_label = labels[0]
                print(f"æ­£åœ¨å¤„ç†ç›®æ ‡åŸŸæ ·æœ¬ {i+1} (ç±»åˆ«: {original_label})...")
                
                input_data = input_data.to(self.device)
                
                # 1. è·å–ç‰¹å¾
                feature_map = self._get_aggregated_map(input_data)
                
                # 2. é¢„æµ‹æ©ç 
                pred_logits = self.segmenter(feature_map)
                
                # 3. åå¤„ç†
                # å°†é¢„æµ‹ç»“æœè°ƒæ•´å›åŸå§‹å›¾åƒå¤§å°
                pred_logits_resized = F.interpolate(pred_logits, size=input_data.shape[2:], mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits_resized).squeeze(0) # [1, H, W]
                
                heatmap_norm = pred_prob.cpu().numpy().squeeze() # [H, W]
                binary_mask = (heatmap_norm > threshold).astype(np.uint8) * 255
                
                # 4. å¯è§†åŒ– (å¤ç”¨æ‚¨çš„ `run_localization` ä¸­çš„å¯è§†åŒ–é€»è¾‘)
                original_image_cv2 = _tensor_to_cv2_image(input_data[0])
                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                output_image_cv2 = original_image_cv2.copy()
                cv2.drawContours(output_image_cv2, contours, -1, (0, 0, 255), 2) 
                
                heatmap_colored = cv2.applyColorMap(
                    (heatmap_norm * 255).astype(np.uint8), 
                    cv2.COLORMAP_JET
                )
                overlay_image = cv2.addWeighted(original_image_cv2, 0.6, heatmap_colored, 0.4, 0)

                # 5. ä¿å­˜
                fig, axes = plt.subplots(1, 4, figsize=(20, 6))
                fig.suptitle(f"Target Sample {i+1} (Label: {original_label}) - Supervised Localization", fontsize=16)
                
                axes[0].imshow(_cv2_image_to_plt(original_image_cv2))
                axes[0].set_title("Original Target Image")
                axes[0].axis('off')

                axes[1].imshow(heatmap_norm, cmap='jet')
                axes[1].set_title(f"Predicted Heatmap (Supervised)")
                axes[1].axis('off')

                axes[2].imshow(_cv2_image_to_plt(overlay_image))
                axes[2].set_title("Heatmap Overlay")
                axes[2].axis('off')
                
                axes[3].imshow(_cv2_image_to_plt(output_image_cv2))
                axes[3].set_title(f"Circled (Thresh={threshold})")
                axes[3].axis('off')
                
                output_filename = os.path.join(output_dir, f"target_{self.config.data.category}_sample_{i+1}.png")
                plt.savefig(output_filename)
                plt.close(fig)
                print(f"âœ… ç›®æ ‡åŸŸå®šä½ç»“æœå·²ä¿å­˜è‡³: {output_filename}")
                
                processed_samples += 1
                
        print(f"--- ğŸ‰ ç›®æ ‡åŸŸå®šä½å®Œæˆ! ç»“æœä¿å­˜åœ¨ '{output_dir}' æ–‡ä»¶å¤¹ ---")

# --- å¯è§†åŒ–å’ŒåŒ…è£…å™¨ä»£ç  (ä¿æŒä¸å˜) ---
def visualize_gradcam_for_transformer(cam, diff_steps, category, sample_index, is_average=False):
    """å°†ä¸€ç»´çš„ CAM ç»“æœå¯è§†åŒ–ä¸ºæ¡å½¢å›¾ã€‚"""
    plt.figure(figsize=(15, 6))
    plt.bar(range(len(cam)), cam, color='skyblue')
    
    font_path = './simhei.ttf' 
    my_font = FontProperties(fname=font_path) if os.path.exists(font_path) else None

    title_prefix = "Average " if is_average else f"Sample {sample_index} "
    plt.xlabel('Denoising Step Index', fontproperties=my_font)
    plt.ylabel('Importance', fontproperties=my_font)
    plt.title(f'Grad-CAM for Transformer ({title_prefix}Category: {category})', fontproperties=my_font)
    
    tick_indices = np.linspace(0, len(diff_steps) - 1, num=min(len(diff_steps), 20), dtype=int)
    plt.xticks(ticks=tick_indices, labels=np.array(diff_steps)[tick_indices], rotation=45, fontsize=8)
    
    plt.grid(True, axis='y')
    plt.tight_layout()
    
    output_dir = 'grad_cam_results'
    os.makedirs(output_dir, exist_ok=True)
    
    file_name_prefix = "average" if is_average else f'sample_{sample_index}'
    k = 0
    while os.path.exists(os.path.join(output_dir, f'{category}_{file_name_prefix}_gradcam_{k}.png')):
        k += 1
    
    output_path = os.path.join(output_dir, f'{category}_{file_name_prefix}_gradcam_{k}.png')
    plt.savefig(output_path)
    plt.close()
    print(f"Grad-CAM å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {output_path}")

class TransformerCamWrapper(nn.Module):
    def __init__(self, model):
        super(TransformerCamWrapper, self).__init__()
        self.model = model

    def forward(self, x):
        # æ¥æ”¶ä¼ªè£…çš„ "å›¾åƒ" è¾“å…¥ [B, Feature_dim, Seq_len, 1]
        # æ¢å¤æˆ Transformer éœ€è¦çš„åºåˆ—å½¢çŠ¶ [B, Seq_len, Feature_dim]
        x_reshaped = x.squeeze(-1).permute(0, 2, 1)
        # æ¨¡å‹çš„è¾“å‡ºæ˜¯ (output, encoded_x)ï¼ŒåŒ…è£…å™¨åº”è¯¥åªè¿”å› Grad-CAM éœ€è¦çš„ logits
        output, _ = self.model(x_reshaped)
        return output

class DDAD_Transformer_Analysis:
    def __init__(self, unet, config) -> None:
        self.unet = unet
        self.config = config
        import dataset
        
        # 2. (å…³é”®!) å¼ºåˆ¶ Python é‡æ–°åŠ è½½è¯¥æ¨¡å—ï¼Œ
        #    æ¸…é™¤ç”± 'unet.py' æˆ–å…¶ä»– 'import *' å¼•èµ·çš„ä»»ä½•æ±¡æŸ“
        importlib.reload(dataset) 
        
        # 3. ç°åœ¨æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°ä»è¿™ä¸ªå¹²å‡€çš„ã€é‡æ–°åŠ è½½çš„æ¨¡å—ä¸­è®¿é—® *ç±»*
        self.test_dataset = dataset.Dataset_maker(
            root=config.data.data_dir, 
            category=config.data.category, 
            config=config, 
            is_train=False
        )
        self.reconstruction = Reconstruction(self.unet, self.config)
        
        input_dim = config.data.image_size * config.data.image_size * config.data.input_channel
        projection_dim = 512 
        seq_length = len(range(0, self.config.model.test_trajectoy_steps, self.config.model.skip))

        self.transformer = ClassificationTransformer(
            input_dim=input_dim, projection_dim=projection_dim, 
            num_heads=8, num_layers=2, num_classes=2, seq_length=seq_length
        ).to(self.config.model.device)

        self.optimizer = torch.optim.AdamW(self.transformer.parameters(), lr=1e-4, weight_decay=1e-4)
        num_good = 0
        num_anomaly = 0
        # if hasattr(self.test_dataset, 'image_files'):
        #     for img_file_path in self.test_dataset.image_files:
        #         parent_dir_name = Path(img_file_path).parent.name
        #         if parent_dir_name == "Normal" or parent_dir_name == "good":
        #             num_good += 1
        #         else:
        #             # å‡è®¾å…¶ä»–æ‰€æœ‰æ–‡ä»¶å¤¹ï¼ˆå¦‚ 'Anomaly', 'scratch', 'crack'ï¼‰éƒ½æ˜¯å¼‚å¸¸
        #             num_anomaly += 1
        if hasattr(self.test_dataset, 'image_files'):
            # æ³¨æ„ï¼šimg_file_info æ˜¯ä¸€ä¸ªå…ƒç»„ (path, label, mask)
            for img_file_info in self.test_dataset.image_files:
                # åªè·å–å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå³è·¯å¾„å­—ç¬¦ä¸²
                img_path_str = img_file_info[0] 
                
                # ç°åœ¨å°†è·¯å¾„å­—ç¬¦ä¸²ä¼ é€’ç»™ Path
                parent_dir_name = Path(img_path_str).parent.name 
                
                # æ‚¨çš„åŸå§‹é€»è¾‘æ˜¯åŸºäºæ–‡ä»¶å¤¹åç§°åŒºåˆ†å¥½/åæ ·æœ¬ã€‚
                # æ—¢ç„¶æ‚¨åœ¨ Dataset_maker ä¸­å·²ç»æœ‰äº† 'good' æˆ– 'defective' æ ‡ç­¾ï¼Œ
                # æ›´å¥½çš„æ–¹æ³•æ˜¯ç›´æ¥ä½¿ç”¨æ ‡ç­¾æ¥è®¡ç®—æ•°é‡ï¼Œé¿å…ä¾èµ–æ–‡ä»¶ç»“æ„ï¼š
                
                label = img_file_info[1] # è·å–æ ‡ç­¾å­—ç¬¦ä¸²
                if label == 'good':
                    num_good += 1
                else:
                    num_anomaly += 1
        # bottle : 229, 63
        # carpet : 308, 89 
        print(f"DEBUG: num_good = {num_good}, num_anomaly = {num_anomaly}")
        total = num_good + num_anomaly
        weight_good, weight_anomaly = total / (2.0 * num_good), total / (2.0 * num_anomaly)
        class_weights = torch.tensor([weight_good, weight_anomaly], device=self.config.model.device)
        print(f"Using class weights: good={weight_good:.2f}, anomaly={weight_anomaly:.2f}")
        self.criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
        self.num_epochs = 15
        clean_data_name = self.config.data.category.split("_")[0]
        self.checkpoint_dir = os.path.join(os.getcwd(), self.config.model.checkpoint_dir, clean_data_name)
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # self.transformer_checkpoint_path = os.path.join(self.checkpoint_dir, 'transformer_cls_checkpoint.pth')
        # self.static_dataset_path = os.path.join(self.checkpoint_dir, 'transformer_static_ethat_dataset.pt')

        # 2. (æ–°) diff_maps (åƒç´ å·®å¼‚) ç‰¹å¾çš„è·¯å¾„
        self.static_diff_maps_dataset_path = os.path.join(self.checkpoint_dir, 'transformer_static_diff_maps_dataset.pt')
        self.transformer_diff_maps_checkpoint_path = os.path.join(self.checkpoint_dir, 'transformer_cls_diff_maps_checkpoint.pth')
        # -----------------

    def _generate_and_save_static_ethat_dataset(self):
        """
        (åŸå‡½æ•°é‡å‘½å)
        ç”Ÿæˆå¹¶ä¿å­˜ et_hat (é¢„æµ‹å™ªå£°) åºåˆ—ã€‚
        æ³¨æ„: è¿™ä¾èµ–äº self.reconstruction è¿”å› (final, et_hats)ã€‚
        """
        print(f"Generating static ET_HAT dataset for Transformer at {self.static_ethat_dataset_path}...")
        all_ethat_sequences, all_labels = [], []
        self.unet.eval()
        loader = DataLoader(self.test_dataset, batch_size=1, shuffle=False, num_workers=self.config.model.num_workers)
        with torch.no_grad():
            for input_data, _, labels in loader:
                input_data = input_data.to(self.config.model.device)
                try:
                    # å‡è®¾ä½ çš„ Reconstruction è¿”å›å€¼æ˜¯ (final_reconstruction, et_hats)
                    _, et_hats = self.reconstruction(input_data, input_data, self.config.model.w)
                    ethat_flat = torch.stack(et_hats, dim=1).view(input_data.size(0), len(et_hats), -1)
                    all_ethat_sequences.append(ethat_flat.cpu())
                    all_labels.extend([0 if l == 'good' else 1 for l in labels])
                except Exception as e:
                    print(f"Error during et_hat generation (skipping batch): {e}")
                    print("This might be due to a mismatch in Reconstruction return values.")
                    continue

        if not all_ethat_sequences:
            print("No et_hat sequences were generated. Aborting.")
            return None, None
            
        all_ethats_tensor = torch.cat(all_ethat_sequences, dim=0)
        all_labels_tensor = torch.tensor(all_labels, dtype=torch.long)
        torch.save({'ethat_sequences': all_ethats_tensor, 'labels': all_labels_tensor}, self.static_ethat_dataset_path)
        print("Static ET_HAT dataset saved successfully.")
        return all_ethats_tensor, all_labels_tensor

    # --- ä½ çš„æ–°å‡½æ•° ---
    def _generate_and_save_static_diff_maps_dataset(self):
        """
        (æ–°å‡½æ•°)
        ç”Ÿæˆå¹¶ä¿å­˜ diff_maps (åƒç´ å·®å¼‚å›¾) åºåˆ—ã€‚
        è¿™ä¾èµ–äº self.reconstruction è¿”å› (..., diff_maps, ...)ã€‚
        """
        print(f"Generating static DIFF_MAPS dataset for Transformer at {self.static_diff_maps_dataset_path}...")
        all_diff_map_sequences, all_labels = [], []
        self.unet.eval()
        loader = DataLoader(self.test_dataset, batch_size=1, shuffle=False, num_workers=self.config.model.num_workers)
        with torch.no_grad():
            for input_data, _, labels in loader:
                input_data = input_data.to(self.config.model.device)
                try:
                    # æ ¹æ®ä½ æä¾›çš„ Reconstruction.py:
                    # è¿”å›: final, images_before, images_after, diff_maps, xs
                    _, _, _, diff_maps, _ = self.reconstruction(input_data, input_data, self.config.model.w)
                    
                    diff_map_flat = torch.stack(diff_maps, dim=1).view(input_data.size(0), len(diff_maps), -1)
                    all_diff_map_sequences.append(diff_map_flat.cpu())
                    all_labels.extend([0 if l == 'good' else 1 for l in labels])
                except Exception as e:
                    print(f"Error during diff_maps generation (skipping batch): {e}")
                    print("Ensure your Reconstruction class returns at least 4 values, with the 4th being diff_maps.")
                    continue
        
        if not all_diff_map_sequences:
            print("No diff_map sequences were generated. Aborting.")
            return None, None

        all_diff_maps_tensor = torch.cat(all_diff_map_sequences, dim=0)
        all_labels_tensor = torch.tensor(all_labels, dtype=torch.long)
        torch.save({'diff_map_sequences': all_diff_maps_tensor, 'labels': all_labels_tensor}, self.static_diff_maps_dataset_path)
        print("Static DIFF_MAPS dataset saved successfully.")
        return all_diff_maps_tensor, all_labels_tensor

    def _train_transformer(self, static_data, static_labels, checkpoint_path):
        """ (ä¿®æ”¹) è®­ç»ƒå‡½æ•°ç°åœ¨æ¥å—ä¸€ä¸ª checkpoint è·¯å¾„ """
        print(f"Starting CLS Transformer training, saving to {checkpoint_path}...")
        static_dataset = TensorDataset(static_data, static_labels)
        train_loader = DataLoader(static_dataset, batch_size=self.config.data.test_batch_size, shuffle=True)
        self.transformer.train()
        for epoch in range(self.num_epochs):
            total_loss = 0
            for data_batch, labels_batch in train_loader:
                data_batch, labels_batch = data_batch.to(self.config.model.device), labels_batch.to(self.config.model.device)
                self.optimizer.zero_grad()
                output, _ = self.transformer(data_batch)
                loss = self.criterion(output, labels_batch)
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{self.num_epochs}, Loss: {total_loss / len(train_loader):.4f}")
        torch.save(self.transformer.state_dict(), checkpoint_path)

    def calculate_recall_at_k(self, labels, probabilities, k):
        # ... æ­¤å‡½æ•°é€»è¾‘ä¸å˜ ...
        sorted_indices = np.argsort(probabilities)[::-1]
        sorted_labels = labels[sorted_indices]
        top_k_labels = sorted_labels[:k]
        recall_at_k = np.sum(top_k_labels == 1) / k
        return recall_at_k

    def __call__(self, force_train=False, use_diff_maps=True) -> Any:
        """
        (ä¿®æ”¹) ä¸»è°ƒç”¨å‡½æ•°ï¼Œå¢åŠ  use_diff_maps å¼€å…³
        """
        
        # --- 1. é€‰æ‹©è¦ä½¿ç”¨çš„æ•°æ®é›†è·¯å¾„ ---
        if use_diff_maps:
            print("--- Mode: Using DIFF_MAPS ---")
            static_dataset_path = self.static_diff_maps_dataset_path
            transformer_checkpoint_path = self.transformer_diff_maps_checkpoint_path
            generate_func = self._generate_and_save_static_diff_maps_dataset
            data_key = 'diff_map_sequences'
        else:
            print("--- Mode: Using ET_HATS ---")
            static_dataset_path = self.static_ethat_dataset_path
            transformer_checkpoint_path = self.transformer_ethat_checkpoint_path
            generate_func = self._generate_and_save_static_ethat_dataset
            data_key = 'ethat_sequences'

        # --- 2. åŠ è½½æˆ–ç”Ÿæˆæ•°æ®é›† ---
        if os.path.exists(static_dataset_path) and not force_train:
            print(f"Loading static dataset from file: {static_dataset_path}")
            dataset_dict = torch.load(static_dataset_path)
            static_data, static_labels = dataset_dict[data_key], dataset_dict['labels']
        else:
            static_data, static_labels = generate_func()
        
        if static_data is None:
            print("Failed to load or generate dataset. Exiting.")
            return

        # --- 3. è®­ç»ƒæˆ–åŠ è½½ Transformer ---
        if force_train or not os.path.exists(transformer_checkpoint_path):
            self._train_transformer(static_data, static_labels, transformer_checkpoint_path)
        else:
            transformer_checkpoint_path = "/data/xjy/DDAD/checkpoints_DAGM/Class1/transformer_cls_diff_maps_checkpoint.pth"
            print(f"Loading pre-trained CLS transformer from {transformer_checkpoint_path}")
            self.transformer.load_state_dict(torch.load(transformer_checkpoint_path, map_location=self.config.model.device))
        
        # --- 4. è¯„ä¼° ---
        print(f"Starting evaluation on {data_key} using CLS Transformer...")
        self.transformer.eval()
        all_predictions, all_probabilities = [], []
        static_eval_dataset = TensorDataset(static_data, static_labels)
        with torch.no_grad():
            for data_batch, _ in DataLoader(static_eval_dataset, batch_size=self.config.data.test_batch_size):
                data_batch = data_batch.to(self.config.model.device)
                preds, _ = self.transformer(data_batch)
                probabilities = torch.softmax(preds, dim=1).cpu().numpy()
                all_predictions.extend(torch.argmax(preds, dim=1).cpu().tolist())
                all_probabilities.extend(probabilities[:, 1])
        
        static_labels_np = static_labels.numpy().flatten()
        try:
            auroc = roc_auc_score(static_labels_np, all_probabilities)
            auprc = average_precision_score(static_labels_np, all_probabilities)
            num_anomalies = np.sum(static_labels_np == 1)
            if num_anomalies > 0:
                recall_at_k = self.calculate_recall_at_k(static_labels_np, np.array(all_probabilities), k=num_anomalies)
                print(f"Recall@{num_anomalies}: {recall_at_k:.4f}")
            print(f"AUROC: {auroc:.4f}")
            print(f"AUPRC: {auprc:.4f}")
        except ValueError as e:
            print(f"Error in metric calculation: {e}")
    
    # --- (ç”¨è¿™ä¸ªç‰ˆæœ¬æ›¿æ¢) ---
    def run_localization(self, num_samples=5, ksize_blur=5, sigma_blur=1.5, threshold=0.5, use_final_residual=False):
        """
        (æ›´æ–°ç‰ˆ) 
        - å¢åŠ äº†ä¸“é—¨ç”¨äºå®šä½çš„ `w_localization` å‚æ•°ã€‚
        - (æ–°) å¢åŠ äº† "Reconstructed Image" çš„å¯è§†åŒ–è¾“å‡ºã€‚
        """
        print(f"--- ğŸš€ å¼€å§‹æ‰§è¡Œå¼‚å¸¸å®šä½ (Anomaly Localization) ---")
        
        # --- (æ–°) ä¸ºå®šä½è®¾ç½®ä¸“é—¨çš„ w å€¼ ---
        # å°è¯• 0.5, 0.2, 0.0ã€‚
        # config.model.w (ä¾‹å¦‚ 4.0) å¯¹äºå®šä½æ¥è¯´å¤ªé«˜äº†ï¼
        w_localization = 0.2 
        print(f"--- (é‡è¦) ä½¿ç”¨ä¸“é—¨çš„ w_localization: {w_localization} ---")
        
        if use_final_residual:
            method_name = f"Final Residual (w={w_localization})"
            print(f"--- ç­–ç•¥: {method_name}")
        else:
            method_name = f"Aggregated DiffMaps (w={w_localization})"
            print(f"--- ç­–ç•¥: {method_name}")
            
        print(f"å°†å¤„ç† {num_samples} ä¸ªæ ·æœ¬...")
        
        loader = DataLoader(self.test_dataset, batch_size=1, shuffle=False, 
                            num_workers=self.config.model.num_workers)
        
        output_dir = 'localization_results'
        os.makedirs(output_dir, exist_ok=True)
        
        self.unet.eval()
        processed_samples = 0
        
        with torch.no_grad():
            for i, (input_data, _, labels) in enumerate(loader):
                if processed_samples >= num_samples:
                    break
                
                original_label = labels[0]
                print(f"æ­£åœ¨å¤„ç†æ ·æœ¬ {i+1} (ç±»åˆ«: {original_label})...")
                
                input_data = input_data.to(self.config.model.device)
                
                # 1. è¿è¡Œé‡å»º (ä½¿ç”¨æˆ‘ä»¬æ–°çš„ w_localization)
                final_recon, _, _, diff_maps, _ = self.reconstruction(
                    input_data, 
                    input_data, 
                    w_localization  # <--- (é‡è¦ä¿®æ”¹)
                )
                
                # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©çƒ­åŠ›å›¾æ¥æº
                if use_final_residual:
                    residual_map = torch.abs(final_recon - input_data)
                    aggregated_map = residual_map.squeeze(0) 
                else:
                    abs_diff_maps = [torch.abs(d) for d in diff_maps]
                    diff_stack = torch.stack(abs_diff_maps, dim=0) 
                    aggregated_map = torch.mean(diff_stack, dim=0)
                    aggregated_map = aggregated_map.squeeze(0) 

                # ... (å¤„ç†é€šé“) ...
                if aggregated_map.shape[0] == 3: 
                    aggregated_map = torch.mean(aggregated_map, dim=0)
                else: 
                    aggregated_map = aggregated_map.squeeze(0) 
                heatmap_raw = aggregated_map.cpu().numpy()
                
                # 3. åå¤„ç†: é«˜æ–¯æ¨¡ç³Š
                heatmap_smooth = gaussian_filter(heatmap_raw, sigma=sigma_blur) 

                # 4. åˆ›å»ºå¹¶åº”ç”¨å¯¹è±¡æ©ç 
                # ... (æ©ç é€»è¾‘ä¿æŒä¸å˜) ...
                original_img_gray_np = input_data[0].cpu().numpy().transpose(1, 2, 0)
                if original_img_gray_np.shape[2] == 3:
                    original_img_gray_np = 0.299 * original_img_gray_np[:,:,0] + \
                                           0.587 * original_img_gray_np[:,:,1] + \
                                           0.114 * original_img_gray_np[:,:,2]
                else:
                    original_img_gray_np = original_img_gray_np.squeeze()
                original_img_gray_np = (original_img_gray_np * 0.5 + 0.5) * 255
                original_img_gray_np = original_img_gray_np.astype(np.uint8)
                _, object_mask = cv2.threshold(original_img_gray_np, 0, 255, 
                                               cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)
                heatmap_masked = heatmap_smooth * (object_mask / 255.0)
                
                # 5. å½’ä¸€åŒ–å’Œé˜ˆå€¼åŒ–
                # ... (å½’ä¸€åŒ–é€»è¾‘ä¿æŒä¸å˜) ...
                map_min, map_max = heatmap_masked.min(), heatmap_masked.max()
                heatmap_norm = (heatmap_masked - map_min) / (map_max - map_min + 1e-6)
                heatmap_norm = heatmap_norm * (object_mask / 255.0)
                binary_mask = (heatmap_norm > threshold).astype(np.uint8) * 255
                
                # 6. å¯è§†åŒ–
                original_image_cv2 = _tensor_to_cv2_image(input_data[0])
                
                # --- (æ–°) å°†é‡å»ºå›¾ä¹Ÿè½¬ä¸º CV2 æ ¼å¼ ---
                final_recon_cv2 = _tensor_to_cv2_image(final_recon[0])
                # ------------------------------------

                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, 
                                                cv2.CHAIN_APPROX_SIMPLE)
                output_image_cv2 = original_image_cv2.copy()
                cv2.drawContours(output_image_cv2, contours, -1, (0, 0, 255), 2) 
                heatmap_colored = cv2.applyColorMap(
                    (heatmap_norm * 255).astype(np.uint8), 
                    cv2.COLORMAP_JET
                )
                overlay_image = original_image_cv2.copy()
                mask_indices = (object_mask == 255)
                overlay_image[mask_indices] = cv2.addWeighted(
                    original_image_cv2[mask_indices], 0.6, 
                    heatmap_colored[mask_indices], 0.4, 0
                )

                # 7. ä¿å­˜ç»˜å›¾ (ä¿®æ”¹ä¸º 5 ä¸ªå­å›¾)
                # --- (ä¿®æ”¹) 1x4 -> 1x5, è°ƒæ•´ figsize ---
                fig, axes = plt.subplots(1, 5, figsize=(25, 6)) 
                fig.suptitle(f"Sample {i+1} - Label: {original_label} - Category: {self.config.data.category}", 
                             fontsize=16)
                
                axes[0].imshow(_cv2_image_to_plt(original_image_cv2))
                axes[0].set_title("Original Image")
                axes[0].axis('off')

                # --- (æ–°) æ·»åŠ é‡å»ºå›¾ ---
                axes[1].imshow(_cv2_image_to_plt(final_recon_cv2))
                axes[1].set_title(f"Reconstructed (w={w_localization})")
                axes[1].axis('off')
                
                # --- (ä¿®æ”¹) ç´¢å¼• +1 ---
                axes[2].imshow(heatmap_norm, cmap='jet')
                axes[2].set_title(f"Heatmap ({method_name})") 
                axes[2].axis('off')
                
                # --- (ä¿®æ”¹) ç´¢å¼• +1 ---
                axes[3].imshow(_cv2_image_to_plt(overlay_image))
                axes[3].set_title("Heatmap Overlay")
                axes[3].axis('off')
                
                # --- (ä¿®æ”¹) ç´¢å¼• +1 ---
                axes[4].imshow(_cv2_image_to_plt(output_image_cv2))
                axes[4].set_title(f"Circled (Thresh={threshold})")
                axes[4].axis('off')
                
                output_filename = os.path.join(output_dir, 
                                               f"{self.config.data.category}_sample_{i+1}_{original_label}.png")
                plt.savefig(output_filename)
                plt.close(fig)
                print(f"âœ… å®šä½ç»“æœå·²ä¿å­˜è‡³: {output_filename}")
                
                processed_samples += 1
        
        print(f"--- ğŸ‰ å®šä½å®Œæˆ! ç»“æœä¿å­˜åœ¨ '{output_dir}' æ–‡ä»¶å¤¹ ---")
    # --- (å°†è¿™ä¸ªæ–°æ–¹æ³•æ·»åŠ åˆ° DDAD_Transformer_Analysis ç±»ä¸­) ---

    def analyze_step_residuals(self, num_samples=5):
        """
        (æ–°) é€æ­¥æ®‹å·® (Residual) åˆ†æã€‚
        
        è¿™ä¸ªæ–¹æ³•åœ¨ DDAD_Transformer_Analysis å†…éƒ¨è¿è¡Œï¼Œ
        å› æ­¤å¯ä»¥æ­£ç¡®è®¿é—®å·²åŠ è½½çš„ self.test_dataset å’Œ self.reconstructionã€‚
        
        æ³¨æ„: ä½ çš„æ—¥å¿—æåˆ°äº† "Epsilon (et_hat)"ï¼Œä½†ä½ çš„ä»£ç 
        (ä¾‹å¦‚ run_localization, _generate_static_diff_maps_dataset)
        å¼ºçƒˆè¡¨æ˜é‡å»ºæ¨¡å—è¿”å›çš„æ˜¯ 'diff_maps' (5ä¸ªè¿”å›å€¼)ã€‚
        
        æ­¤å‡½æ•°å°†éµå¾ªä½ çš„ä»£ç å®ç°ï¼Œåˆ†æ 'diff_maps'ã€‚
        """
        print("--- ğŸš€ (å·²ç§»å…¥) å¼€å§‹é€æ­¥ Residual (DiffMap) åˆ†æ ---")
        
        # 1. ä½¿ç”¨å·²ç»åŠ è½½çš„æ•°æ®é›†
        # self.test_dataset å·²ç»åœ¨ __init__ ä¸­è¢«æ­£ç¡®åŠ è½½
        if not self.test_dataset:
            print("--- âŒ é”™è¯¯: self.test_dataset æœªåˆå§‹åŒ– ---")
            return
            
        print(f"æ­£åœ¨ä½¿ç”¨å·²åŠ è½½çš„æ•°æ®é›†: {self.config.data.category}")
        
        # 2. å‡†å¤‡ Dataloader
        loader = DataLoader(self.test_dataset, batch_size=1, shuffle=False, 
                            num_workers=self.config.model.num_workers)
        
        # 3. ç¡®ä¿ unet åœ¨è¯„ä¼°æ¨¡å¼
        self.unet.eval()
        
        print(f"å°†åˆ†æ {num_samples} ä¸ªæ ·æœ¬...")
        
        output_dir = 'residual_analysis_results'
        os.makedirs(output_dir, exist_ok=True)
        
        processed_samples = 0
        
        with torch.no_grad():
            for i, (input_data, _, labels) in enumerate(loader):
                if processed_samples >= num_samples:
                    break
                
                original_label = labels[0]
                print(f"--- æ­£åœ¨åˆ†ææ ·æœ¬ {i+1} (ç±»åˆ«: {original_label}) ---")
                
                input_data = input_data.to(self.config.model.device)
                
                try:
                    # 4. è¿è¡Œé‡å»ºä»¥è·å– diff_maps
                    # (è¿™éµå¾ªäº†ä½  run_localization ä¸­çš„ 5 è¿”å›å€¼ç»“æ„)
                    final_recon, _, _, diff_maps, _ = self.reconstruction(
                        input_data, 
                        input_data, 
                        self.config.model.w # ä½¿ç”¨é…ç½®ä¸­çš„ w
                    ) 
                    
                    print(f"  è·å–äº† {len(diff_maps)} ä¸ª diff_mapsã€‚")

                    # 5. (ç¤ºä¾‹åˆ†æ) å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ diff_maps æ¼”å˜
                    if processed_samples < 3: # ä¸ºå‰ 3 ä¸ªæ ·æœ¬ç»˜åˆ¶
                        print(f"  æ­£åœ¨ä¸ºæ ·æœ¬ {i+1} ç”Ÿæˆå¯è§†åŒ–å›¾...")
                        
                        # (å¤ç”¨ run_localization ä¸­çš„å¯è§†åŒ–è¾…åŠ©å‡½æ•°)
                        original_image_cv2 = _tensor_to_cv2_image(input_data[0])
                        final_recon_cv2 = _tensor_to_cv2_image(final_recon[0])
                        
                        # å›¾è¡¨å¸ƒå±€: (åŸå›¾, é‡å»ºå›¾, diff_1, ..., diff_N, èšåˆå›¾)
                        num_plots = 2 + len(diff_maps) + 1
                        fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 3, 4))
                        if num_plots == 1: # ç¡®ä¿åœ¨ num_plots=1 æ—¶ axes æ˜¯å¯è¿­ä»£çš„
                            axes = [axes]
                            
                        fig.suptitle(f"Step-wise Residual (DiffMap) æ¼”å˜ - æ ·æœ¬ {i+1} ({original_label})")

                        axes[0].imshow(_cv2_image_to_plt(original_image_cv2))
                        axes[0].set_title("Original")
                        axes[0].axis('off')
                        
                        axes[1].imshow(_cv2_image_to_plt(final_recon_cv2))
                        axes[1].set_title(f"Final Recon (w={self.config.model.w})")
                        axes[1].axis('off')

                        all_diffs_np = []
                        for step_idx, diff_map in enumerate(diff_maps):
                            # (B, C, H, W) -> (H, W)
                            diff_map_viz = torch.mean(torch.abs(diff_map[0]), dim=0).cpu().numpy()
                            all_diffs_np.append(diff_map_viz)
                            
                            ax = axes[step_idx + 2]
                            im = ax.imshow(diff_map_viz, cmap='viridis')
                            ax.set_title(f"Diff Step {step_idx}")
                            ax.axis('off')
                        
                        # èšåˆå›¾
                        aggregated_map_np = np.mean(np.stack(all_diffs_np, axis=0), axis=0)
                        ax = axes[-1]
                        im = ax.imshow(aggregated_map_np, cmap='viridis')
                        ax.set_title("Aggregated DiffMap")
                        ax.axis('off')

                        output_filename = os.path.join(output_dir, f"diff_map_analysis_{self.config.data.category}_sample_{i+1}.png")
                        plt.savefig(output_filename)
                        plt.close(fig)
                        print(f"  âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {output_filename}")

                    # 6. (ç¤ºä¾‹åˆ†æ) è®¡ç®— diff_maps çš„ L2 èŒƒæ•°
                    diff_map_norms = [torch.norm(d[0]) for d in diff_maps]
                    print(f"  diff_map èŒƒæ•° (L2 Norms): {[f'{n:.2f}' for n in diff_map_norms]}")
                    
                    processed_samples += 1
                    
                except Exception as e:
                    print(f"--- âŒ é”™è¯¯: åœ¨åˆ†ææ ·æœ¬ {i+1} æ—¶å‡ºé”™ ---")
                    print(f"  åŸå§‹é”™è¯¯: {e}")
                    print("  è¯·ç¡®ä¿ self.reconstruction() è¿”å› 5 ä¸ªå€¼ (final, _, _, diff_maps, _)")
                    break # åœæ­¢å¾ªç¯

        print(f"--- ğŸ‰ æ­¥éª¤åˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨ '{output_dir}' æ–‡ä»¶å¤¹ ---")
        